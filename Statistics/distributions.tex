\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{multicol}
\usepackage{svg}
\usepackage{caption}
\usepackage{vmargin}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{epigraph}

\theoremstyle{definition}
\newtheorem{defi}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}{Properties}[section]
\newtheorem{lemma}{Lemma}[section]

\title{Probability Distributions}
\author{Xiaozhe Yao\footnote{https://yaonotes.org/lecture-notes}}
\date{\today}
\begin{document}

\maketitle

\section{Basic Concepts}

\begin{defi}
\textbf{Probability Distribution} In probability theory and stats, a probability distribution is a mathematical function that provides the probabilities of occurrence of different possible outcomes in an experiment. More technically, the probability distribution is a description of a random phenomenon in terms of the probabilities of events.
\end{defi}

\begin{defi}
\textbf{Sample Space} A probability distribution is specified in terms of an underlying \textit{sample space}, which is the set of all possible outcomes of the random phenomenon being observed. 
\end{defi}

\begin{defi}
\textbf{Types of Distribution} 

By different outcomes, probability distributions are generally divided into two classes:\begin{itemize}
    \item \textit{Discrete Probability Distribution}, where the set of possible outcomes is discrete and can be encoded by a discrete list of the probabilities of the outcomes, known as a probability mass function (PMF).
    \item \textit{Continuous Probability Distribution}, where the set of possible outcomes can take on values in a continuous range, and is typically described by probability density functions (with the probability of any individual outcome actually being 0).
\end{itemize} 

By different sample spaces, probability distributions can be divided into the following types:
\begin{itemize}
    \item \textit{Univariate}: Sample Space is one dimensional, such as real numbers, list of labels, ordered labels or binary. It gives the probabilities of a single random variable taking on various alternative values.
    \item \textit{Multivariate}: Sample space is a vector space of dimension $2$ or more. It gives the probabilities of a random vector - a list of two or more random variables.
\end{itemize}
\end{defi}

\begin{defi}
\textbf{Cumulative Distribution Function (CDF)} The CDF of a real-valued random variable $\textbf{X}$, or just the distrbution function of $\textbf{X}$, evaluated at $x$, is the probability that $\textbf{X}$ will take a value less than or equal to $x$.
\end{defi}

\section{Bernoulli Distribution}
\begin{defi}
\textbf{Bernoulli Distribution} is a discrete probability distribution of a random variable which takes the value $1$ with probability $p$ and the value $0$ with probability $q=1-p$.
\end{defi}
\begin{prop} There are some fundamental facts for Bernoulli Distribution:
\begin{itemize}
    \item If $\textbf{X}$ is a random variable with this distribution, then $$Pr(X=1)=p=1-Pr(X=0)=1-q$$.
    \item The probability mass function $f$ of this distribution, over possible outcomes $k$, is $$f(k;p)=\begin{cases}
               p & \text{if k=1}\\
               1-p & \text{if k=0} \\
            \end{cases}$$ It can be expressed as $$f(k;p)=p^k(1-p)^{(1-k)}, k\in\{0,1\}$$ or as $$f(k;p)=pk+(1-p)(1-k)$$
    \item The expectation $E(X)=\sum_{x}P(X=x)=1\times p + 0\times (1-p)=p$.
\end{itemize}
\end{prop}
\begin{theorem}
\textbf{The expectation and variance theorem}. 
    \begin{itemize}
        \item $E(XY)=E(X)E(Y)$.
        \item $Var(X+Y)=Var(X)+Var(Y)$.
    \end{itemize}
\begin{proof}
As below:
\begin{itemize}
    \item $E(XY)=\sum_{x}\sum_{y}P(X=x, Y=y)$.
\end{itemize}
\end{proof}
\end{theorem}
\end{document}