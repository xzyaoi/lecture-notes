\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{multicol}
\usepackage{svg}
\usepackage{caption}
\usepackage{vmargin}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{epigraph}

\theoremstyle{definition}
\newtheorem{defi}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}{Properties}[section]
\newtheorem{lemma}{Lemma}[section]

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\title{Probability Distributions}
\author{Xiaozhe Yao\footnote{https://yaonotes.org/lecture-notes}}
\date{\today}
\begin{document}

\maketitle

\section{Basic Concepts}

\begin{defi}
\textbf{Probability Distribution} In probability theory and stats, a probability distribution is a mathematical function that provides the probabilities of occurrence of different possible outcomes in an experiment. More technically, the probability distribution is a description of a random phenomenon in terms of the probabilities of events.
\end{defi}

\begin{defi}
\textbf{Sample Space} A probability distribution is specified in terms of an underlying \textit{sample space}, which is the set of all possible outcomes of the random phenomenon being observed. 
\end{defi}

\begin{defi}
\textbf{Types of Distribution} 

By different outcomes, probability distributions are generally divided into two classes:\begin{itemize}
    \item \textit{Discrete Probability Distribution}, where the set of possible outcomes is discrete and can be encoded by a discrete list of the probabilities of the outcomes, known as a probability mass function (PMF).
    \item \textit{Continuous Probability Distribution}, where the set of possible outcomes can take on values in a continuous range, and is typically described by probability density functions (with the probability of any individual outcome actually being 0).
\end{itemize} 

By different sample spaces, probability distributions can be divided into the following types:
\begin{itemize}
    \item \textit{Univariate}: Sample Space is one dimensional, such as real numbers, list of labels, ordered labels or binary. It gives the probabilities of a single random variable taking on various alternative values.
    \item \textit{Multivariate}: Sample space is a vector space of dimension $2$ or more. It gives the probabilities of a random vector - a list of two or more random variables.
\end{itemize}
\end{defi}

\begin{defi}
\textbf{Cumulative Distribution Function (CDF)} The CDF of a real-valued random variable $\textbf{X}$, or just the distrbution function of $\textbf{X}$, evaluated at $x$, is the probability that $\textbf{X}$ will take a value less than or equal to $x$.
\end{defi}

\section{Bernoulli Distribution}
\begin{defi}
\textbf{Bernoulli Distribution} is a discrete probability distribution of a random variable which takes the value $1$ with probability $p$ and the value $0$ with probability $q=1-p$.
\end{defi}
\begin{prop} There are some fundamental facts for Bernoulli Distribution:
\begin{itemize}
    \item If $\textbf{X}$ is a random variable with this distribution, then $$Pr(X=1)=p=1-Pr(X=0)=1-q$$.
    \item The probability mass function $f$ of this distribution, over possible outcomes $k$, is $$f(k;p)=\begin{cases}
               p & \text{if k=1}\\
               1-p & \text{if k=0} \\
            \end{cases}$$ It can be expressed as $$f(k;p)=p^k(1-p)^{(1-k)}, k\in\{0,1\}$$ or as $$f(k;p)=pk+(1-p)(1-k)$$
    \item The expectation $E(X)=\sum_{x}xP(X=x)=1\times p + 0\times (1-p)=p$.
    \item The variance is $Var(X)=p(1-p)$. To prove, simply plug $E(X)$ in it: $Var(X)=E(X^2)-E(X)^2=p-p^2=p(1-p)$.
\end{itemize}
\end{prop}
\begin{theorem}
\textbf{The expectation and variance for two variables}. Assume $X,Y$ are two independent variables.
    \begin{itemize}
        \item $E(XY)=E(X)E(Y)$.
        \item $Var(X+Y)=Var(X)+Var(Y)$.
    \end{itemize}
\begin{proof}
As below:
\begin{itemize}
    \item $E(XY)=xy\sum_{x}\sum_{y}P(X=x, Y=y)$, as $X,Y$ are independent to each other, $$P(X=x,Y=y)=P(X=x)P(Y=y)$$ thus $$\sum_{x}\sum_{y}xyP(X=x, Y=y)=\sum_{x}xP(X=x)\sum_{y}yP(Y=y)=E(X)E(Y)$$
    \item $Var(X+Y)\\=E((X+Y)^2)-E(X+Y)^2\\=E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\=E(X^2)+E(Y^2)+2E(XY)-E(X)^2-E(Y)^2-2E(X)E(Y)$, 
    
    by using the fact that $E(XY)=E(X)E(Y)$, we continue
    
    $Var(X+Y)\\=E(X^2)-E(X)^2+E(Y^2)-E(Y)^2\\=Var(X)+Var(Y)$.
\end{itemize}
\end{proof}
\end{theorem}

\textit{Note:} Bernoulli Distribution is a special case of binomial distribution, with $n=1$.

\section{Binomial Distribution}

\begin{defi}
\textbf{Binomial Distribution}. Binomial Distribution with parameters $n$ and $p$ is the discrete probability distribution of the number of successes in a sequence of $n$. Each asking a single yes/no question and gives boolean-valued outcome. That is, A single experiment is a \textit{Bernoulli experiment}, or named \textit{Bernoulli trial}. A sequence of outcomes is called a Bernoulli process. 

If the random variable $X$ follows the binomial distribution with parameters $n\in \mathbb{N}$ and $p\in[0,1]$, we write it as $X\sim B(n,p)$.
\end{defi}

\begin{prop}
\textbf{PMF} The probability of getting exactly $k$ successes in $n$ independent Bernoulli trials is given by the probability mass function: $$f(k,n,p)=Pr(k;n,p)=Pr(X=k)=C_{n}^{k}p^k(1-p)^{n-k}, \: C_{n}^{k}=\frac{n!}{k!(n-k)!}$$

The formula can be understood as: $k$ successes occur with probability $p^k$ and $n-k$ failures occur with probability $(1-p)^{n-k}$. Then the successes can occur anywhere among the $n$ trials, and therefore there are $C_{n}^{k}$ different ways of distributing $k$ successes in a sequence of $n$ trials.

\textbf{Monotonicity} There is always an integer M such that $f(k,n,p)$ is monotone increasing for $k<M$, and monotone decreasing for $k>M$
When calculating the PMF, we can safely calculate only $\frac{n}{2}$, by the definition of binomial coefficients, we have $f(k,n,p)=f(n-k,n,1-p)$.

\begin{figure}[h]
\caption{Probability Mass Function for Binomial Distribution}
\centering
\includegraphics[width=200pt]{Statistics/images/Binomial_distribution_pmf.pdf}
\end{figure}

\end{prop}

\begin{prop}
\textbf{CDF} The cumulative distribution function can be expressed as $F(k,n,p)=Pr(X\leq k)=\sum_{i=0}^{\floor*{k}}C_{n}^{i}p^i(1-p)^{n-i}$, where $\floor*{k}$ refers to the greatest integer less than or equal to $k$.
\begin{figure}[h]
\caption{Cumulative Distribution Function for Binomial Distribution}
\centering
\includegraphics[width=200pt]{Statistics/images/Binomial_distribution_cdf.pdf}
\end{figure}

\end{prop}
\begin{prop}
\textbf{Expectation} If $X\sim B(n,p)$, then $E(X)=np$.
\begin{proof}
This follows from the linearity of the expected value along with fact that $X$ is the sum of $n$ identical, independent Bernoulli random variables, each with expected value $p$, i.e. $X=X_1+X_2+...+X_n=\sum_{i=1}^{n}X_i$. Thus $E(X)=E(\sum_{i=1}^{n}X_i)=E(X_1+...+X_n)=E(X_1)+...E(X_n)=\sum_{i=1}^{n}E(X_i)=\sum_{i=1}^{n}p=np$.
\end{proof}
\end{prop}

\begin{prop}
\textbf{Variance} If $X\sim B(n,p)$, then $Var(X)=np(1-p)$.
\begin{proof}
It also follows from the fact that the variance of a sum of independent random variables is the sum of the variances.
$Var(X)=Var(\sum_{i=1}^{n}X_i)=Var(X_1+...+X_n)=Var(X_1)+...+Var(X_n)=\sum_{i=1}^{n}Var(X_i)=np(1-p)$.
\end{proof}
\end{prop}

\subsection{Hypergeometric Distribution}

In Binomial Distribution, every trial is independent to each other. In some cases, the previous trials may have some impact on the future trials. For example, there are $n$ objects, and $k$ objects with the property $\mathcal{P}$, we take the objects randomly, then the probability for ``get an object with the property $\mathcal{P}$" is $p=\frac{k}{n}$. We do this experiment one by one.
\begin{itemize}
    \item If we return the object everytime we complete a single trial, then every trial is identical and independent to each other. It is binomial distribution.
    \item If we \textbf{DONT} return, then every trial will not be independent and identical, and in this case it is called hypergeometric distribution.
\end{itemize}

\begin{defi}
\textbf{Hypergeometric Distribution} Hypergeometric Distribution is a discrete probability distribution that describes the probability of $k$ successes (random draws for which the object drawn has a specified feature) in $n$ draws, \textbf{without replacement}. The process is from a finite population size $N$ that contains $K$ objects with that feature.
\end{defi}

\begin{prop}
\textbf{PMF} The following conditions characterize the hypergeometric distribution:
\begin{itemize}
    \item The result of each draw is binary, just the same as in binomial distribution.
    \item The probability of a success changes on each draw, because the population decreases after every single trial.
\end{itemize}

A random variable follows the hypergeometric distribution if its probability mass function is given by $P_X(k)=Pr(X=k)=\frac{C_K^{k}C_{N-K}^{n-k}}{C_N^n}$, where \begin{itemize}
    \item $N$ is the population size.
    \item $K$ is the number of success states in the population.
    \item $n$ is the number of draws, i.e. quantity drawn in each trial.
    \item $k$ is the number of observed successes.
\end{itemize}
\begin{figure}[h]
\caption{Probability Mass Function for Hypergeometric Distribution}
\centering
\includegraphics[width=200pt]{Statistics/images/hyper_geometric_pmf.pdf}
\end{figure}
\end{prop}

\begin{prop}
\textbf{CDF}: The cumulative distribution function for Hypergeometric distribution is defined as $$F(M,K,N)=\sum_{i=0}^{x}\frac{C_i^{k}C_{N-i}^{n-k}}{C_N^n}$$
\begin{figure}[h]
\caption{Cumulative Distribution Function for Hypergeometric Distribution}
\centering
\includegraphics[width=200pt]{Statistics/images/hyper_geometric_cdf.pdf}
\end{figure}
\end{prop}

\textit{Note:} From the graphs comparing CDF/PMF of Binomial Distribution and Hypergeometric Distribution, we see they are quite similar \textbf{when the sample population is large}.

\section{Poisson Distribution}
\begin{defi}
\textbf{Poisson Distribution} Poisson Distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. It is useful when we want to model \textbf{times of events} in a certain spatial or temporal interval.

When a random variable follows Poisson distribution, we write it as $X\sim P(\lambda)$ or $X\sim \pi(\lambda)$ where $\lambda$ represents the frequency of the event.

The Poisson distribution is an appropriate model if the following assumptions are true:
\begin{itemize}
    \item $k$ is the number of times an event occurs in an interval, $k\in\mathbb{N}_{>0}$.
    \item The occurrence of one event does not affect the probability that a second event will occur. i.e. events occur independently.
    \item The average rate at which events occur is independent of any occurrences. Usually, it is assumed to be constant, but may vary with time in practice.
    \item Two events cannot occur at exactly the same instant; instead, at each very small sub-interval exactly one event either occurs or does not occur.
\end{itemize}

For example, the Poisson distribution may be applied to the following events:
\begin{itemize}
    \item The number of meteorites greater than 1 meter diameter that strike Earth in a year.
    \item The number of patients arriving in an emergency room between 10 and 11 pm.
    \item The number of laser photons hitting a detector in a particular time interval.
\end{itemize}

The Poisson distribution is the limit of binomial distribution, for which the probability of success for each trial equals to $\lambda$ divided by the number of trials, as the number of trials approaches infinity.

\end{defi}

\begin{defi}
\textbf{PMF} A discrete random variable $X$ is said to have a Poisson distribution with parameter $\lambda>0$, if for $k=0,1,2,...$, the probability mass function of $X$ is given by $f(k;\lambda)=Pr(X=k)=\frac{\lambda^ke^{-\lambda}}{k!}$. It means the probability of observing $k$ events in an interval where $\lambda$ is the average number of events per interval.
\begin{figure}[h]
\caption{Probability Mass Function for Poisson Distribution}
\centering
\includegraphics[width=200pt]{Statistics/images/Poisson_pmf.pdf}
\end{figure}
\end{defi}

\begin{prop}
If $X\sim \pi(\lambda)$, then it has the following properties:
\begin{itemize}
    \item $E(X)=\lambda$
    \begin{proof}
    $E(X)=\sum_{x=0}^{\infty}xP(X=x)$
    \end{proof}
    \item $Var(X)=\lambda$
\end{itemize}
\end{prop}

\subsection{Exponential Distribution}
\section{Normal Distribution}
\end{document}